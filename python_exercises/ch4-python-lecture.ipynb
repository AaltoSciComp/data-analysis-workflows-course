{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4\n",
    "\n",
    "This notebook contains the commands that are shown in the lecture 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different resources involved in data analysis pipelines\n",
    "\n",
    "### Processors as a resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time taken:\n",
      "\n",
      "For loop: 4.6\n",
      "Vectorized operation: 0.0056\n",
      "\n",
      "Speedup: 815\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_zeros = 10000\n",
    "ntimes = 1000\n",
    "\n",
    "z = np.zeros(n_zeros)\n",
    "\n",
    "time_for_1 = time.time()\n",
    "for t in range(ntimes):\n",
    "    for i in range(n_zeros):\n",
    "        z[i] = z[i] + 1\n",
    "time_for_2 = time.time()\n",
    "\n",
    "time_for = time_for_2-time_for_1\n",
    "\n",
    "z = np.zeros(n_zeros)\n",
    "\n",
    "time_vec_1 = time.time()\n",
    "for t in range(ntimes):\n",
    "    z = z + 1\n",
    "time_vec_2 = time.time()\n",
    "\n",
    "time_vec = time_vec_2-time_vec_1\n",
    "\n",
    "print(\"\"\"\n",
    "Time taken:\n",
    "\n",
    "For loop: %.2g\n",
    "Vectorized operation: %.2g\n",
    "\n",
    "Speedup: %.0f\n",
    "\"\"\" % (time_for, time_vec, time_for/time_vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAM as a resource\n",
    "\n",
    "Bootstrapping pipeline from chapter 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_filesizes(filesizes_file):\n",
    "    filesizes = pd.read_table(filesizes_file, sep='\\s+', names=['Bytes','MonthsTo2021', 'Files'])\n",
    "    \n",
    "    # Remove empty files\n",
    "    filesizes = filesizes[filesizes.loc[:,'Bytes'] != 0]\n",
    "    # Create a column for log2 of bytes\n",
    "    filesizes['BytesLog2'] = np.log2(filesizes.loc[:, 'Bytes'])\n",
    "    filesizes.loc[:,'BytesLog2'] = filesizes.loc[:,'BytesLog2'].astype(np.int64)\n",
    "    # Determine total space S used by N files of size X during date D: S=N*X \n",
    "    filesizes['SpaceUsage'] = filesizes.loc[:,'Bytes']*filesizes.loc[:,'Files']\n",
    "    # Determine file year and month from the MonthsTo2021-column\n",
    "    filesizes['TotalMonths'] = 2021*12 - filesizes['MonthsTo2021'] - 1\n",
    "    filesizes['Year'] = filesizes['TotalMonths'] // 12\n",
    "    filesizes['Month'] = filesizes['TotalMonths'] % 12 + 1\n",
    "    filesizes['Day'] = 1\n",
    "    \n",
    "    # Set year for really old files and files with incorrect timestamps\n",
    "    invalid_years = (filesizes['Year'] < 2010) | (filesizes['Year'] > 2020)\n",
    "    filesizes.loc[invalid_years, ['Year','Month']] = np.NaN\n",
    "    \n",
    "    # Get month names for the correct ordering of Month categories\n",
    "    month_names = pd.date_range(start='2000-01', freq='M', periods=12).month_name()\n",
    "    # Create Date\n",
    "    filesizes['Date'] = pd.to_datetime(filesizes[['Year', 'Month', 'Day']])\n",
    "    # Set Month\n",
    "    filesizes['Month'] = pd.Categorical(filesizes['Date'].dt.month_name(), categories=month_names, ordered=True)\n",
    "    # Set Month to be an ordered categorical with predefined levels \n",
    "    filesizes['Month'] = pd.Categorical(filesizes['Month'], categories=month_names, ordered=True)\n",
    "    # Sort data based on Date and BytesLog2\n",
    "    filesizes.sort_values(['Date','BytesLog2'], inplace=True)\n",
    "    # Remove old columns\n",
    "    filesizes.drop(['MonthsTo2021','TotalMonths', 'Day'], axis=1, inplace=True)\n",
    "    return filesizes\n",
    "\n",
    "def aggregate_filesize_data(data, groupings, targets, agg_function):\n",
    "    # Drop rows with NaNs (invalid years)\n",
    "    data_relevant = data.dropna(axis=0)\n",
    "    # Pick relevant columns\n",
    "    data_relevant = data_relevant.loc[:, groupings + targets]\n",
    "    # Change grouping to category for prettier plotting\n",
    "    data_relevant[groupings] = data_relevant[groupings].astype('category')\n",
    "\n",
    "    # Aggregate data\n",
    "    data_aggregated = data_relevant.groupby(groupings).agg(agg_function).reset_index()\n",
    "    return data_aggregated\n",
    "\n",
    "def get_bootstrapped_means(dataset, target_col=None, weight_col=None, n_means=1000):\n",
    "    # Pick relevant columns\n",
    "    df = dataset[[target_col, weight_col]].copy()\n",
    "    # Pick target data column\n",
    "    target_data = df[target_col]\n",
    "    # Pick weight data column\n",
    "    weight_data = df[weight_col]\n",
    "    # Fill zeros to those byte sizes that are not present in the Files-data\n",
    "    weight_data.fillna(0, inplace=True)\n",
    "    # Normalize weight_data into probabilities\n",
    "    weight_data = weight_data/weight_data.sum()\n",
    "    \n",
    "    # Create means vector\n",
    "    means = np.zeros(n_means, dtype=np.float64)\n",
    "    for i in range(n_means):\n",
    "        # Calculate resampled mean\n",
    "        means[i] = np.mean(np.random.choice(target_data, 100, replace=True, p=weight_data))\n",
    "\n",
    "    return means\n",
    "\n",
    "def bootstrap_byteslog2_mean(dataset, group_variable, target_variable, n_means=1000):\n",
    "\n",
    "    bootstrapping_function = lambda x: get_bootstrapped_means(x, 'BytesLog2', target_variable, n_means=n_means)\n",
    "\n",
    "    bootstrapped_means = dataset.groupby(group_variable).apply(lambda x: pd.Series({'data': x}))\n",
    "    bootstrapped_means['SampledMeans'] = bootstrapped_means['data'].apply(bootstrapping_function)\n",
    "    bootstrapped_means['Mean'] = bootstrapped_means['SampledMeans'].apply(np.mean)\n",
    "    bootstrapped_means.drop('data', axis=1, inplace=True)\n",
    "    return bootstrapped_means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010.0</td>\n",
       "      <td>12.97741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011.0</td>\n",
       "      <td>14.04096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012.0</td>\n",
       "      <td>10.67631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013.0</td>\n",
       "      <td>13.41275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014.0</td>\n",
       "      <td>14.05066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year      Mean\n",
       "0  2010.0  12.97741\n",
       "1  2011.0  14.04096\n",
       "2  2012.0  10.67631\n",
       "3  2013.0  13.41275\n",
       "4  2014.0  14.05066"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chapter3_pipeline(n_means=10000):\n",
    "    \n",
    "    filesizes = load_filesizes('../data/filesizes_timestamps.txt')\n",
    "\n",
    "    yearly_bytes_sum = aggregate_filesize_data(filesizes, ['Year','BytesLog2'], ['Files', 'SpaceUsage'], 'sum')\n",
    "\n",
    "    bootstrapped_yearly_means = bootstrap_byteslog2_mean(yearly_bytes_sum, 'Year', 'Files', n_means=n_means)\n",
    "    \n",
    "    bootstrapped_yearly_means = bootstrapped_yearly_means.reset_index()[['Year','Mean']]\n",
    "\n",
    "    return bootstrapped_yearly_means\n",
    "\n",
    "chapter3_pipeline(n_means=100).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index         69520\n",
      "Bytes         69520\n",
      "Files         69520\n",
      "BytesLog2     69520\n",
      "SpaceUsage    69520\n",
      "Year          69520\n",
      "Month          9768\n",
      "Date          69520\n",
      "dtype: int64\n",
      "Index          128\n",
      "Year           881\n",
      "BytesLog2     2097\n",
      "Files         3784\n",
      "SpaceUsage    3784\n",
      "dtype: int64\n",
      "\n",
      "Original data: 496408 bytes\n",
      "Summarized data: 10674 bytes\n",
      "\n",
      "Reduction ratio: 46.51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filesizes = load_filesizes('../data/filesizes_timestamps.txt')\n",
    "yearly_bytes_sum = aggregate_filesize_data(filesizes, ['Year','BytesLog2'], ['Files', 'SpaceUsage'], 'sum')\n",
    "\n",
    "print(filesizes.memory_usage(deep=True))\n",
    "print(yearly_bytes_sum.memory_usage(deep=True))\n",
    "\n",
    "filesizes_size = filesizes.memory_usage(deep=True).sum()\n",
    "summarized_size = yearly_bytes_sum.memory_usage(deep=True).sum()\n",
    "print(\"\"\"\n",
    "Original data: %d bytes\n",
    "Summarized data: %d bytes\n",
    "\n",
    "Reduction ratio: %.2f\n",
    "\"\"\" % (filesizes_size, summarized_size, filesizes_size/summarized_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'memory_scope_variable' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6d1e9e06eb99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmemory_scope_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_scope_variable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'memory_scope_variable' is not defined"
     ]
    }
   ],
   "source": [
    "def memory_scope_test():\n",
    "    \n",
    "    memory_scope_variable = np.random.random(1000)\n",
    "    print(memory_scope_variable.nbytes)\n",
    "\n",
    "memory_scope_test()\n",
    "print(memory_scope_variable.nbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000000062 1.0000000000000397\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "def memtest_nocollect(n=1000):\n",
    "\n",
    "    A = np.random.random(n**2)\n",
    "    \n",
    "    A_mean = np.mean(A)\n",
    "    \n",
    "    time.sleep(5)\n",
    "    \n",
    "    B = np.matrix(np.random.random((n, n)) + A_mean)\n",
    "    B = B + B.T\n",
    "    B_inv = np.linalg.inv(B)\n",
    "    \n",
    "    return np.max(B*B_inv)\n",
    "\n",
    "def memtest_collect(n=1000):\n",
    "\n",
    "    A = np.random.random(n**2)\n",
    "    \n",
    "    A_mean = np.mean(A)\n",
    "    \n",
    "    del A\n",
    "    gc.collect()\n",
    "    \n",
    "    time.sleep(5)\n",
    "    \n",
    "    B = np.matrix(np.random.random((n, n)) + A_mean)\n",
    "    B = B + B.T\n",
    "    B_inv = np.linalg.inv(B)\n",
    "    \n",
    "    return np.max(B*B_inv)\n",
    "\n",
    "print(memtest_nocollect(100), memtest_collect(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 419.71 MiB, increment: 307.42 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit memtest_nocollect(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 418.88 MiB, increment: 273.54 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit memtest_collect(3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelization strategies\n",
    "### Using internal parallelization provided by libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time taken:\n",
      "\n",
      "1 thread: 4.07\n",
      "4 threads: 1.61\n",
      "\n",
      "Speedup: 2.53\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import mkl\n",
    "\n",
    "A = np.random.random((4000,4000))\n",
    "\n",
    "A = A*A.T\n",
    "\n",
    "mkl.set_num_threads(1)\n",
    "\n",
    "time_1thread_1 = time.time()\n",
    "np.linalg.inv(A)\n",
    "time_1thread_2 = time.time()\n",
    "\n",
    "time_1thread = time_1thread_2 - time_1thread_1\n",
    "\n",
    "mkl.set_num_threads(4)\n",
    "\n",
    "time_4thread_1 = time.time()\n",
    "np.linalg.inv(A)\n",
    "time_4thread_2 = time.time()\n",
    "\n",
    "time_4thread = time_4thread_2 - time_4thread_1\n",
    "\n",
    "print(\"\"\"\n",
    "Time taken:\n",
    "\n",
    "1 thread: %.2f\n",
    "4 threads: %.2f\n",
    "\n",
    "Speedup: %.2f\n",
    "\"\"\" % (time_1thread, time_4thread, time_1thread/time_4thread))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiprocessing\n",
    "\n",
    "#### Doing parallel maps with multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   x\n",
      "0  1\n",
      "1  2\n",
      "2  3\n",
      "3  4\n",
      "4  5\n",
      "   x   y\n",
      "0  1   1\n",
      "1  2   4\n",
      "2  3   9\n",
      "3  4  16\n",
      "4  5  25\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "def x_squared(x):\n",
    "    return x*x\n",
    "\n",
    "data = pd.DataFrame({'x':range(1,101)})\n",
    "\n",
    "print(data.head())\n",
    "\n",
    "# Run mapping with parallel pool\n",
    "with Pool(4) as parallel_pool:\n",
    "    y = parallel_pool.map(x_squared, data['x'])\n",
    "\n",
    "# Convert resulting list into a Series\n",
    "y_series = pd.Series(y, name='y')\n",
    "\n",
    "# Add series to data\n",
    "data['y'] = y_series\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original pipeline: 13.21\n",
      "      Year       Mean\n",
      "0   2010.0  12.972783\n",
      "1   2011.0  14.041408\n",
      "2   2012.0  10.677178\n",
      "3   2013.0  13.410552\n",
      "4   2014.0  14.042002\n",
      "5   2015.0  11.745106\n",
      "6   2016.0  13.542875\n",
      "7   2017.0  11.981502\n",
      "8   2018.0  13.279947\n",
      "9   2019.0  13.707754\n",
      "10  2020.0  13.229183\n",
      "Time taken by 1 workers: 12.30 Speedup was: 1.07\n",
      "Maximum difference between calculated means: 0.009673000000001153\n",
      "Time taken by 2 workers: 7.42 Speedup was: 1.78\n",
      "Maximum difference between calculated means: 0.008904000000001133\n",
      "Time taken by 3 workers: 4.74 Speedup was: 2.79\n",
      "Maximum difference between calculated means: 0.008904000000001133\n",
      "Time taken by 4 workers: 3.90 Speedup was: 3.38\n",
      "Maximum difference between calculated means: 0.008904000000001133\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "def chapter3_pipeline_parallel(n_means=1000, n_workers=1):\n",
    "\n",
    "    filesizes = load_filesizes('../data/filesizes_timestamps.txt')\n",
    "    yearly_bytes_sum = aggregate_filesize_data(filesizes, ['Year','BytesLog2'], ['Files', 'SpaceUsage'], 'sum')\n",
    "\n",
    "    bootstrapped_means = yearly_bytes_sum.groupby('Year').apply(lambda x: pd.Series({'data': x}))\n",
    "    \n",
    "    # Actual parallel part\n",
    "    \n",
    "    \"\"\"\n",
    "    Here we use functools.partial to create a function with partially filled\n",
    "    arguments because multiprocessing.Pool.map does not work that well with\n",
    "    lambda-functions. get_bootstrapped_means was changed to allow target_col\n",
    "    and weight_col to be set with keyword arguments so that the arguments are\n",
    "    given in correct order.\n",
    "    \"\"\"\n",
    "    bootstrapping_function = functools.partial(get_bootstrapped_means, target_col='BytesLog2', weight_col='Files', n_means=n_means)\n",
    "\n",
    "    # Initialize a parallel pool with n_workers workers\n",
    "    with Pool(n_workers) as parallel_pool:\n",
    "        # Map a function to each dataset. Output is a list of ndarrays.\n",
    "        sampled_means = parallel_pool.map(bootstrapping_function, bootstrapped_means['data'])\n",
    "    \n",
    "    # Convert list of ndarrays into a Series of ndarrays   \n",
    "    sampled_means = pd.Series(sampled_means, name='SampledMeans', index=bootstrapped_means.index)\n",
    "\n",
    "    # Place Series into our DataFrame\n",
    "    bootstrapped_means['SampledMeans'] = sampled_means\n",
    "    # End of the parallel part\n",
    "\n",
    "    bootstrapped_means['Mean'] = bootstrapped_means['SampledMeans'].apply(np.mean)\n",
    "    \n",
    "    bootstrapped_means = bootstrapped_means.reset_index()[['Year','Mean']]\n",
    "\n",
    "    return(bootstrapped_means)\n",
    "\n",
    "# Measure performance and verify results \n",
    "time1 = time.time()\n",
    "means_orig = chapter3_pipeline(n_means=10000)\n",
    "time2 = time.time()\n",
    "orig_time = time2-time1\n",
    "print('Original pipeline: %.2f' % (orig_time))\n",
    "print(means_orig)\n",
    "\n",
    "for n_workers in range(1,5):\n",
    "    time1 = time.time()\n",
    "    means = chapter3_pipeline_parallel(n_means=10000, n_workers=n_workers)\n",
    "    time2 = time.time()\n",
    "    \n",
    "    print('Time taken by %d workers: %.2f Speedup was: %.2f' % (n_workers, time2 - time1, orig_time/(time2-time1)))\n",
    "    print('Maximum difference between calculated means:', (means['Mean']-means_orig['Mean']).abs().max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing code with profilers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         17987532 function calls (17324700 primitive calls) in 17.112 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "   List reduced from 1380 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "   110000    4.397    0.000   14.884    0.000 {method 'choice' of 'numpy.random.mtrand.RandomState' objects}\n",
      "   330370    1.257    0.000    1.257    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "110123/110121    1.036    0.000    6.070    0.000 algorithms.py:1616(take_nd)\n",
      "551172/331084    0.735    0.000    7.628    0.000 {built-in method numpy.array}\n",
      "   110121    0.498    0.000    1.710    0.000 algorithms.py:1487(_get_take_nd_function)\n",
      "   110011    0.493    0.000    1.482    0.000 _methods.py:143(_mean)\n",
      "  2553898    0.457    0.000    0.643    0.000 {built-in method builtins.isinstance}\n",
      "   110088    0.411    0.000    1.590    0.000 cast.py:442(maybe_promote)\n",
      "   110062    0.363    0.000    0.363    0.000 {pandas._libs.algos.take_1d_int64_int64}\n",
      "   220504    0.358    0.000    1.160    0.000 _dtype.py:321(_name_get)\n",
      "   551555    0.358    0.000    0.948    0.000 common.py:1460(is_extension_array_dtype)\n",
      "   440191    0.347    0.000    0.347    0.000 generic.py:5123(__getattr__)\n",
      "   110040    0.298    0.000    0.946    0.000 fromnumeric.py:70(_wrapreduction)\n",
      "   110026    0.296    0.000    6.837    0.000 categorical.py:1241(__array__)\n",
      "   551553    0.292    0.000    0.444    0.000 base.py:413(find)\n",
      "   110011    0.291    0.000    0.353    0.000 _methods.py:59(_count_reduce_items)\n",
      "   110144    0.285    0.000    0.285    0.000 {pandas._libs.algos.ensure_int64}\n",
      "  2096198    0.255    0.000    0.255    0.000 {built-in method builtins.issubclass}\n",
      "   220004    0.225    0.000    8.471    0.000 series.py:750(__array__)\n",
      "       11    0.223    0.020   16.967    1.542 <ipython-input-3-f6247b6d056d>:47(get_bootstrapped_means)\n",
      "\n",
      "\n",
      "Top methods by cumulative time:\n",
      "\n",
      "   110000    4.397    0.000   14.884    0.000 {method 'choice' of 'numpy.random.mtrand.RandomState' objects}\n",
      "   330370    1.257    0.000    1.257    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "   110173    0.064    0.000    0.399    0.000 {method 'any' of 'numpy.ndarray' objects}\n",
      "   220315    0.095    0.000    0.095    0.000 {method 'format' of 'str' objects}\n",
      "   220266    0.077    0.000    0.077    0.000 {method 'get' of 'dict' objects}\n",
      "   110135    0.064    0.000    0.064    0.000 {method 'view' of 'numpy.ndarray' objects}\n",
      "   110070    0.020    0.000    0.020    0.000 {method 'items' of 'dict' objects}\n",
      "        1    0.000    0.000    0.009    0.009 {method 'get_indexer' of 'pandas._libs.index.BaseMultiIndexCodesEngine' objects}\n",
      "        2    0.004    0.002    0.004    0.002 {method 'get_indexer_non_unique' of 'pandas._libs.index.IndexEngine' objects}\n",
      "        1    0.003    0.003    0.003    0.003 {method 'read' of 'pandas._libs.parsers.TextReader' objects}\n",
      "       48    0.001    0.000    0.002    0.000 {method 'get_indexer' of 'pandas._libs.index.IndexEngine' objects}\n",
      "       29    0.000    0.000    0.000    0.000 {method 'sum' of 'numpy.ndarray' objects}\n",
      "       18    0.000    0.000    0.000    0.000 {method 'argsort' of 'numpy.ndarray' objects}\n",
      "       40    0.000    0.000    0.000    0.000 {method 'max' of 'numpy.ndarray' objects}\n",
      "       90    0.000    0.000    0.000    0.000 {method 'copy' of 'numpy.ndarray' objects}\n",
      "       80    0.000    0.000    0.000    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
      "        4    0.000    0.000    0.000    0.000 {method 'get_labels_groupby' of 'pandas._libs.hashtable.Int64HashTable' objects}\n",
      "     1419    0.000    0.000    0.000    0.000 {method 'replace' of 'str' objects}\n",
      "       27    0.000    0.000    0.000    0.000 {method 'all' of 'numpy.ndarray' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {method 'factorize' of 'pandas._libs.hashtable.Int64HashTable' objects}\n"
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "\n",
    "# Initiate profiler\n",
    "pr = cProfile.Profile(subcalls=False)\n",
    "pr.enable()\n",
    "\n",
    "# Run the pipeline\n",
    "chapter3_pipeline(n_means=10000)\n",
    "\n",
    "# Stop profiling\n",
    "pr.disable()\n",
    "\n",
    "# Print stats by total time used (top 20)\n",
    "ps = pstats.Stats(pr).strip_dirs().sort_stats('tottime')\n",
    "ps.print_stats(20)\n",
    "\n",
    "# Print into a StringIO buffer and find top 20 function calls by cumulative time\n",
    "io_stream = io.StringIO()\n",
    "ps_methods = pstats.Stats(pr, stream=io_stream).strip_dirs().sort_stats('cumulative')\n",
    "ps_methods.print_stats()\n",
    "\n",
    "method_lines = [ line for line in io_stream.getvalue().split('\\n') if ' {method' in line ]\n",
    "\n",
    "print('Top methods by cumulative time:\\n')\n",
    "print('\\n'.join(method_lines[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized version of the bootstrapping code\n",
    "def get_bootstrapped_means(dataset, target_col=None, weight_col=None, n_means=1000):\n",
    "    # Pick relevant columns\n",
    "    df = dataset[[target_col, weight_col]].copy()\n",
    "    # Pick target data column\n",
    "    target_data = df[target_col]\n",
    "    # Pick weight data column\n",
    "    weight_data = df[weight_col]\n",
    "    # Fill zeros to those byte sizes that are not present in the Files-data\n",
    "    weight_data.fillna(0, inplace=True)\n",
    "    # Normalize weight_data into probabilities\n",
    "    weight_data = weight_data/weight_data.sum()\n",
    "    \n",
    "    # Calculate resampled mean\n",
    "    means = np.mean(np.random.choice(target_data, 100*n_means, replace=True, p=weight_data).reshape(100,n_means), axis=0)\n",
    "\n",
    "    return means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
