{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4\n",
    "\n",
    "This notebook contains the commands that are shown in the lecture 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different resources involved in data analysis pipelines\n",
    "\n",
    "### Processors as a resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time taken:\n",
      "\n",
      "For loop: 4.5\n",
      "Vectorized operation: 0.0056\n",
      "\n",
      "Speedup: 801\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_zeros = 10000\n",
    "times = 1000\n",
    "\n",
    "z = np.zeros(n_zeros)\n",
    "\n",
    "time_for_1 = time.time()\n",
    "for t in range(times):\n",
    "    for i in range(n_zeros):\n",
    "        z[i] = z[i] + 1\n",
    "time_for_2 = time.time()\n",
    "\n",
    "time_for = time_for_2-time_for_1\n",
    "\n",
    "z = np.zeros(n_zeros)\n",
    "\n",
    "time_vec_1 = time.time()\n",
    "for times in range(times):\n",
    "    z = z + 1\n",
    "time_vec_2 = time.time()\n",
    "\n",
    "time_vec = time_vec_2-time_vec_1\n",
    "\n",
    "print(\"\"\"\n",
    "Time taken:\n",
    "\n",
    "For loop: %.2g\n",
    "Vectorized operation: %.2g\n",
    "\n",
    "Speedup: %.0f\n",
    "\"\"\" % (time_for, time_vec, time_for/time_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_filesizes(filesizes_file):\n",
    "    filesizes = pd.read_table(filesizes_file, sep='\\s+', names=['Bytes','MonthsTo2021', 'Files'])\n",
    "    \n",
    "    # Remove empty files\n",
    "    filesizes = filesizes[filesizes.loc[:,'Bytes'] != 0]\n",
    "    # Create a column for log2 of bytes\n",
    "    filesizes['BytesLog2'] = np.log2(filesizes.loc[:, 'Bytes'])\n",
    "    filesizes.loc[:,'BytesLog2'] = filesizes.loc[:,'BytesLog2'].astype(np.int64)\n",
    "    # Determine total space S used by N files of size X during date D: S=N*X \n",
    "    filesizes['SpaceUsage'] = filesizes.loc[:,'Bytes']*filesizes.loc[:,'Files']\n",
    "    # Determine file year and month from the MonthsTo2021-column\n",
    "    filesizes['TotalMonths'] = 2021*12 - filesizes['MonthsTo2021'] - 1\n",
    "    filesizes['Year'] = filesizes['TotalMonths'] // 12\n",
    "    filesizes['Month'] = filesizes['TotalMonths'] % 12 + 1\n",
    "    filesizes['Day'] = 1\n",
    "    \n",
    "    # Set year for really old files and files with incorrect timestamps\n",
    "    invalid_years = (filesizes['Year'] < 2010) | (filesizes['Year'] > 2020)\n",
    "    filesizes.loc[invalid_years, ['Year','Month']] = np.NaN\n",
    "    \n",
    "    # Get month names for the correct ordering of Month categories\n",
    "    month_names = pd.date_range(start='2000-01', freq='M', periods=12).month_name()\n",
    "    # Create Date\n",
    "    filesizes['Date'] = pd.to_datetime(filesizes[['Year', 'Month', 'Day']])\n",
    "    # Set Month\n",
    "    filesizes['Month'] = pd.Categorical(filesizes['Date'].dt.month_name(), categories=month_names, ordered=True)\n",
    "    # Set Month to be an ordered categorical with predefined levels \n",
    "    filesizes['Month'] = pd.Categorical(filesizes['Month'], categories=month_names, ordered=True)\n",
    "    # Sort data based on Date and BytesLog2\n",
    "    filesizes.sort_values(['Date','BytesLog2'], inplace=True)\n",
    "    # Remove old columns\n",
    "    filesizes.drop(['MonthsTo2021','TotalMonths', 'Day'], axis=1, inplace=True)\n",
    "    return filesizes\n",
    "\n",
    "def aggregate_filesize_data(data, groupings, targets, agg_function):\n",
    "    # Drop rows with NaNs (invalid years)\n",
    "    data_relevant = data.dropna(axis=0)\n",
    "    # Pick relevant columns\n",
    "    data_relevant = data_relevant.loc[:, groupings + targets]\n",
    "    # Change grouping to category for prettier plotting\n",
    "    data_relevant[groupings] = data_relevant[groupings].astype('category')\n",
    "\n",
    "    # Aggregate data\n",
    "    data_aggregated = data_relevant.groupby(groupings).agg(agg_function).reset_index()\n",
    "    return data_aggregated\n",
    "\n",
    "def get_bootstrapped_means(dataset, target_col, weight_col, n_means=1000):\n",
    "    # Pick relevant columns\n",
    "    df = dataset[[target_col, weight_col]].copy()\n",
    "    # Pick target data column\n",
    "    target_data = df[target_col]\n",
    "    # Pick weight data column\n",
    "    weight_data = df[weight_col]\n",
    "    # Fill zeros to those byte sizes that are not present in the Files-data\n",
    "    weight_data.fillna(0, inplace=True)\n",
    "    # Normalize weight_data into probabilities\n",
    "    weight_data = weight_data/weight_data.sum()\n",
    "    \n",
    "    # Create means vector\n",
    "    means = np.zeros(n_means, dtype=np.float64)\n",
    "    for i in range(n_means):\n",
    "        # Calculate resampled mean\n",
    "        means[i] = np.mean(np.random.choice(target_data, 100, replace=True, p=weight_data))\n",
    "\n",
    "    return means\n",
    "\n",
    "def bootstrap_byteslog2_mean(dataset, group_variable, target_variable, n_means=1000):\n",
    "\n",
    "    bootstrapping_function = lambda x: get_bootstrapped_means(x, 'BytesLog2', target_variable, n_means=n_means)\n",
    "\n",
    "    bootstrapped_means = dataset.groupby(group_variable).apply(lambda x: pd.Series({'data': x}))\n",
    "    bootstrapped_means['SampledMeans'] = bootstrapped_means['data'].apply(bootstrapping_function)\n",
    "    bootstrapped_means['Mean'] = bootstrapped_means['SampledMeans'].apply(np.mean)\n",
    "    bootstrapped_means.drop('data', axis=1, inplace=True)\n",
    "    return bootstrapped_means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chapter3_pipeline(n_means=10000):\n",
    "    \n",
    "    filesizes = load_filesizes('../data/filesizes_timestamps.txt')\n",
    "\n",
    "    yearly_bytes_sum = aggregate_filesize_data(filesizes, ['Year','BytesLog2'], ['Files', 'SpaceUsage'], 'sum')\n",
    "\n",
    "    bootstrapped_yearly_means = bootstrap_byteslog2_mean(yearly_bytes_sum, 'Year', 'Files', n_means=n_means)\n",
    "    \n",
    "    bootstrapped_yearly_means = bootstrapped_yearly_means.reset_index()[['Year','Mean']]\n",
    "\n",
    "    return bootstrapped_yearly_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010.0</td>\n",
       "      <td>12.9741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011.0</td>\n",
       "      <td>14.0293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012.0</td>\n",
       "      <td>10.7615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013.0</td>\n",
       "      <td>13.3932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014.0</td>\n",
       "      <td>14.0477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2015.0</td>\n",
       "      <td>11.7602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>13.5082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2017.0</td>\n",
       "      <td>11.8654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>13.2088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2019.0</td>\n",
       "      <td>13.6830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2020.0</td>\n",
       "      <td>13.2385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Year     Mean\n",
       "0   2010.0  12.9741\n",
       "1   2011.0  14.0293\n",
       "2   2012.0  10.7615\n",
       "3   2013.0  13.3932\n",
       "4   2014.0  14.0477\n",
       "5   2015.0  11.7602\n",
       "6   2016.0  13.5082\n",
       "7   2017.0  11.8654\n",
       "8   2018.0  13.2088\n",
       "9   2019.0  13.6830\n",
       "10  2020.0  13.2385"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chapter3_pipeline(n_means=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         17987532 function calls (17324700 primitive calls) in 17.248 seconds\n",
      "\n",
      "   Ordered by: internal time\n",
      "   List reduced from 1380 to 20 due to restriction <20>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "   110000    4.415    0.000   15.015    0.000 {method 'choice' of 'numpy.random.mtrand.RandomState' objects}\n",
      "   330370    1.279    0.000    1.279    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "110123/110121    1.011    0.000    6.131    0.000 algorithms.py:1616(take_nd)\n",
      "551172/331084    0.741    0.000    7.693    0.000 {built-in method numpy.array}\n",
      "   110121    0.524    0.000    1.788    0.000 algorithms.py:1487(_get_take_nd_function)\n",
      "   110011    0.485    0.000    1.495    0.000 _methods.py:143(_mean)\n",
      "  2553898    0.472    0.000    0.660    0.000 {built-in method builtins.isinstance}\n",
      "   110088    0.385    0.000    1.572    0.000 cast.py:442(maybe_promote)\n",
      "   220504    0.368    0.000    1.208    0.000 _dtype.py:321(_name_get)\n",
      "   110062    0.368    0.000    0.368    0.000 {pandas._libs.algos.take_1d_int64_int64}\n",
      "   440191    0.350    0.000    0.350    0.000 generic.py:5123(__getattr__)\n",
      "   551555    0.345    0.000    0.932    0.000 common.py:1460(is_extension_array_dtype)\n",
      "   110026    0.313    0.000    6.898    0.000 categorical.py:1241(__array__)\n",
      "   110040    0.296    0.000    0.923    0.000 fromnumeric.py:70(_wrapreduction)\n",
      "   551553    0.291    0.000    0.445    0.000 base.py:413(find)\n",
      "   110011    0.288    0.000    0.349    0.000 _methods.py:59(_count_reduce_items)\n",
      "   110144    0.286    0.000    0.286    0.000 {pandas._libs.algos.ensure_int64}\n",
      "  2096198    0.263    0.000    0.263    0.000 {built-in method builtins.issubclass}\n",
      "   220004    0.228    0.000    8.559    0.000 series.py:750(__array__)\n",
      "   440682    0.213    0.000    0.306    0.000 numerictypes.py:286(issubclass_)\n",
      "\n",
      "\n",
      "Top methods by cumulative time:\n",
      "\n",
      "   110000    4.415    0.000   15.015    0.000 {method 'choice' of 'numpy.random.mtrand.RandomState' objects}\n",
      "   330370    1.279    0.000    1.279    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "   110173    0.062    0.000    0.400    0.000 {method 'any' of 'numpy.ndarray' objects}\n",
      "   220315    0.099    0.000    0.099    0.000 {method 'format' of 'str' objects}\n",
      "   220266    0.083    0.000    0.083    0.000 {method 'get' of 'dict' objects}\n",
      "   110135    0.065    0.000    0.065    0.000 {method 'view' of 'numpy.ndarray' objects}\n",
      "   110070    0.018    0.000    0.018    0.000 {method 'items' of 'dict' objects}\n",
      "        1    0.000    0.000    0.009    0.009 {method 'get_indexer' of 'pandas._libs.index.BaseMultiIndexCodesEngine' objects}\n",
      "        2    0.004    0.002    0.005    0.002 {method 'get_indexer_non_unique' of 'pandas._libs.index.IndexEngine' objects}\n",
      "        1    0.002    0.002    0.003    0.003 {method 'read' of 'pandas._libs.parsers.TextReader' objects}\n",
      "       48    0.001    0.000    0.001    0.000 {method 'get_indexer' of 'pandas._libs.index.IndexEngine' objects}\n",
      "       18    0.000    0.000    0.000    0.000 {method 'argsort' of 'numpy.ndarray' objects}\n",
      "       40    0.000    0.000    0.000    0.000 {method 'max' of 'numpy.ndarray' objects}\n",
      "       29    0.000    0.000    0.000    0.000 {method 'sum' of 'numpy.ndarray' objects}\n",
      "       90    0.000    0.000    0.000    0.000 {method 'copy' of 'numpy.ndarray' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {method 'factorize' of 'pandas._libs.hashtable.Int64HashTable' objects}\n",
      "       80    0.000    0.000    0.000    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
      "        4    0.000    0.000    0.000    0.000 {method 'get_labels_groupby' of 'pandas._libs.hashtable.Int64HashTable' objects}\n",
      "       27    0.000    0.000    0.000    0.000 {method 'all' of 'numpy.ndarray' objects}\n",
      "     1419    0.000    0.000    0.000    0.000 {method 'replace' of 'str' objects}\n"
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "import io\n",
    "\n",
    "# Initiate profiler\n",
    "pr = cProfile.Profile(subcalls=False)\n",
    "pr.enable()\n",
    "\n",
    "# Run the pipeline\n",
    "chapter3_pipeline(n_means=10000)\n",
    "\n",
    "# Stop profiling\n",
    "pr.disable()\n",
    "\n",
    "# Print stats by total time used (top 20)\n",
    "ps = pstats.Stats(pr).strip_dirs().sort_stats('tottime')\n",
    "ps.print_stats(20)\n",
    "\n",
    "# Print into a StringIO buffer and find top 20 function calls by cumulative time\n",
    "io_stream = io.StringIO()\n",
    "ps_methods = pstats.Stats(pr, stream=io_stream).strip_dirs().sort_stats('cumulative')\n",
    "ps_methods.print_stats()\n",
    "\n",
    "method_lines = [ line for line in io_stream.getvalue().split('\\n') if ' {method' in line ]\n",
    "\n",
    "print('Top methods by cumulative time:\\n')\n",
    "print('\\n'.join(method_lines[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import mkl\n",
    "\n",
    "A = np.random.random((4000,4000))\n",
    "\n",
    "A = A*A.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blas_mkl_info:\n",
      "    libraries = ['mkl_rt', 'pthread']\n",
      "    library_dirs = ['/u/59/tuomiss1/unix/conda/envs/dataanalysis/lib']\n",
      "    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\n",
      "    include_dirs = ['/u/59/tuomiss1/unix/conda/envs/dataanalysis/include']\n",
      "blas_opt_info:\n",
      "    libraries = ['mkl_rt', 'pthread']\n",
      "    library_dirs = ['/u/59/tuomiss1/unix/conda/envs/dataanalysis/lib']\n",
      "    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\n",
      "    include_dirs = ['/u/59/tuomiss1/unix/conda/envs/dataanalysis/include']\n",
      "lapack_mkl_info:\n",
      "    libraries = ['mkl_rt', 'pthread']\n",
      "    library_dirs = ['/u/59/tuomiss1/unix/conda/envs/dataanalysis/lib']\n",
      "    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\n",
      "    include_dirs = ['/u/59/tuomiss1/unix/conda/envs/dataanalysis/include']\n",
      "lapack_opt_info:\n",
      "    libraries = ['mkl_rt', 'pthread']\n",
      "    library_dirs = ['/u/59/tuomiss1/unix/conda/envs/dataanalysis/lib']\n",
      "    define_macros = [('SCIPY_MKL_H', None), ('HAVE_CBLAS', None)]\n",
      "    include_dirs = ['/u/59/tuomiss1/unix/conda/envs/dataanalysis/include']\n",
      "\n",
      "Time taken:\n",
      "\n",
      "1 thread: 4.15\n",
      "4 threads: 1.52\n",
      "\n",
      "Speedup: 2.74\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.show_config()\n",
    "\n",
    "mkl.set_num_threads(1)\n",
    "\n",
    "time_1thread_1 = time.time()\n",
    "np.linalg.inv(A)\n",
    "time_1thread_2 = time.time()\n",
    "\n",
    "time_1thread = time_1thread_2 - time_1thread_1\n",
    "\n",
    "mkl.set_num_threads(4)\n",
    "\n",
    "time_4thread_1 = time.time()\n",
    "np.linalg.inv(A)\n",
    "time_4thread_2 = time.time()\n",
    "\n",
    "time_4thread = time_4thread_2 - time_4thread_1\n",
    "\n",
    "print(\"\"\"\n",
    "Time taken:\n",
    "\n",
    "1 thread: %.2f\n",
    "4 threads: %.2f\n",
    "\n",
    "Speedup: %.2f\n",
    "\"\"\" % (time_1thread, time_4thread, time_1thread/time_4thread))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
