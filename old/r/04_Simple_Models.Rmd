---
title: An R Markdown document converted from "04_Simple_Models.ipynb"
output: html_document
---

# Simple Models

One rarely wants to just view data or summarize it. This session goes through some example models. We'll be using the `broom`-package later, so let's install it now.

```{r}
library(tidyverse)
if (!file.exists('rlibs')) {
    dir.create('rlibs')
}
if (!file.exists('rlibs/broom')) {
    install.packages('broom', repos="http://cran.r-project.org", lib='rlibs')
}
library(broom, lib.loc='rlibs')
library(modelr)
```

## Linear models

Simple linear models are a common starting point for modelling. In R the function `lm` describes a linear model [[lm]](https://www.rdocumentation.org/packages/stats/versions/3.6.1/topics/lm). The first argument for this function is a `formula` that describes the relationship between the predictors and response.

For example, `y ~ x1 + x2` would mean that `y = a*x1 + b*x2 + c`, whereas `y ~ x1 * x2` would mean that the equation would be similar to `y = a*x1 + b*x2 + c*x1*x2 + d`.

Let's try running a linear model with the `mtcars`-dataset we used previously.

```{r}
data(mtcars)

mtcars_tbl <- as_tibble(rownames_to_column(mtcars,var='model'))

str(mtcars_tbl)
```

Previously we plotted `qsec` as a a function of `hp` and plotted a linear fit into it.

```{r}
mtcars_tbl %>%
    ggplot(aes(x=hp,y=qsec)) +
    geom_point() +
    geom_smooth(method=lm)
```

Let's do this now manually. Base-R's function `summary` can be used to view the parameters and a lot of calculated statistics [[summary]](https://www.rdocumentation.org/packages/base/versions/3.6.1/topics/summary).

If you want store the output from multiple models or use the resulting values in your plots, you can use the function `tidy` from `broom` to convert the output into tibbles [[tidy]](https://broom.tidyverse.org/reference/tidy.lm.html) [[broom]](https://broom.tidyverse.org/articles/broom.html).

```{r}
cars_model <- lm(qsec ~ hp, data=mtcars_tbl)

summary(cars_model)

tidy(cars_model)
```

If you want to add these values to the original dataset, you can use `augment` from `broom` or, if you're just interested on predicted values, `add_predictions` from `modelr` [[augment]](https://broom.tidyverse.org/reference/augment.lm.html) [[add_predictions]](https://modelr.tidyverse.org/reference/add_predictions.html).,

```{r}
cars_model %>%
    augment(mtcars_tbl) %>%
    head()
```

```{r}
mtcars_modelled <- mtcars_tbl %>%
    add_predictions(cars_model)
head(mtcars_modelled)
```

Let's plot the data:

```{r}
mtcars_modelled %>%
    ggplot(aes(x=hp, y=qsec)) +
        geom_point() +
        geom_line(color='blue', size=1, aes(y=pred))
```

This plot is missing the confidence intervals. We can get them from `predict` [[predict]](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/predict.lm.html).

```{r}
cars_model_prediction <- as_tibble(predict(cars_model, newdata=mtcars_modelled, interval = 'confidence', level=0.95))
head(cars_model_prediction)
```

`predict` provides the data in a messy format, so adding the upper and lower bounds to our data frame can be a bit messy.

```{r}
mtcars_modelled <- mtcars_modelled %>%
    add_column(
        lower=cars_model_prediction$lwr,
        upper=cars_model_prediction$upr
    )
head(mtcars_modelled)
```

Now we can add confidence intervals to the plot. This exercise shows how much `ggplot2` does for you.

```{r}
mtcars_modelled %>%
    ggplot(aes(x=hp, y=qsec)) +
        geom_point() +
        geom_line(color='blue', size=1, aes(y=pred)) +
        geom_smooth(aes(y=pred, ymin=lower, ymax=upper), stat='identity')
```

## Simple clustering

Let's use simple `kmeans`-clustering in the `iris`-dataset [[kmeans]](https://www.rdocumentation.org/packages/stats/versions/3.6.1/topics/kmeans).

```{r}
data(iris)

iris_tbl <- as_tibble(iris)

print(iris_tbl)
```

```{r}
clusters <- iris_tbl %>%
    select(-Species) %>%
    kmeans(centers=3)
clusters
```

Again, we can `augment` the original dataset.

```{r}
iris_clusters <- clusters %>%
    augment(iris_tbl)
head(iris_clusters)
```

Let's rename the species levels to match the `.cluster`, so that we can find out how well we succeeded.

```{r}
iris_clusters <- iris_clusters %>%
    mutate(species_code = fct_recode(Species, "2"="setosa", "3"="virginica", "1"="versicolor"))
```

```{r}
iris_clusters %>%
    filter(.cluster != species_code)
```

Let's plot the ones that failed.

```{r}
iris_clusters <- iris_clusters %>%
    mutate(clustering_failed=.cluster != species_code)
```

```{r}
iris_clusters %>%
    ggplot(aes(x=Petal.Width, y=Petal.Length, color=.cluster)) +
        geom_point(shape=1)
iris_clusters %>%
    ggplot(aes(x=Sepal.Width, y=Sepal.Length, color=.cluster)) +
        geom_point(shape=1)
```

```{r}
iris_clusters %>%
    ggplot(aes(x=Petal.Width, y=Petal.Length, color=clustering_failed)) +
        geom_point(shape=1)
iris_clusters %>%
    ggplot(aes(x=Sepal.Width, y=Sepal.Length, color=clustering_failed)) +
        geom_point(shape=1)
```

# Exercises

## 1.

Try to improve on the linear model that we used by adding more predictors. What is the best fit you can obtain?

## 2.

Use the subset of our `beer_recipes`-dataset and the `kmeans`-function to cluster American IPAs and American Light Lagers. Try different predictors and check if some predictors are better at clustering than others [[kmeans]](https://www.rdocumentation.org/packages/stats/versions/3.6.1/topics/kmeans).

```{r}
library(tidyverse)
if (!file.exists('rlibs')) {
    dir.create('rlibs')
}
if (!file.exists('rlibs/feather')) {
    install.packages('feather', repos="http://cran.r-project.org", lib='rlibs')
}
library(feather, lib.loc='rlibs')

Sys.setlocale('LC_ALL','C')

beer_recipes <- read_feather('beer_recipes.feather')
beer_recipes <- beer_recipes %>%
    filter(Style == 'American IPA' | Style == 'American Light Lager') %>%
    select(Style:Color, -`Size(L)`) %>%
    mutate_if(is.character, as.factor)

str(beer_recipes)
```

